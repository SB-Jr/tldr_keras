{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras callbacks can help you fix bugs more quickly, and can help you build better models. They can help you visualize how your model’s training is going, and can even help prevent overfitting by implementing early stopping or customizing the learning rate on each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras callbacks return information from a training algorithm while training is taking place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLogger and History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get these without doing anything special — they’re applied to the model by default. By convention, most of the time you’ll just call model.fit()on a model object without assigning the result to anything. Instead of calling model.fit(), you can write my_history = model.fit(). The my_history variable is assigned a keras.callbacks.History object. The historyproperty of this object is a dict with average accuracy and average loss information for each epoch. You can also inspect the paramsproperty, which is a dict of the parameters used to fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback will save your model as a checkpoint file (in hdf5 format) to disk after each successful epoch. You can actually set the output file to be dynamically named based on the epoch. You can also write either the loss value or accuracy value as part of the log’s file name. This is especially handy for models where epochs take an extremely long time, or if you’re running them on AWS spot instances and the spot instance price rises above your maximum bid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t writes a CSV log file containing information about epochs, accuracy, and loss to disk so you can inspect it later. It’s great if you want to roll your own graphs or keep a record of your model training process over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technique to reduce overfitting in neural networks is to use early stopping. Early stopping prevents overtraining of your model by terminating the training process if it’s not really learning anything. This is pretty flexible — you can control what metric to monitor, how much it needs to change to be considered “still learning”, and how many epochs in a row it can falter before the model stops training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RemoteMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback sends JSON status messages via an HTTP POST method. This could be easily integrated with a pub/sub messaging service like Kafka or a queue like Amazon SQS. For instance, you could setup a lightweight HTTP endpoint and route any POSTed JSON as a payload to an SQS queue. You could then have another process monitor the queue to kick off other processes or to handle specific events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method is to start with a relatively large value, and decrease it in later training epochs. All you need to do is write a simple function that returns the desired learning rate based on the current epoch, and pass it as the schedule parameter to this callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a TensorBoard callback, logs will be written to a directory that you can then examine with TensorFlow’s excellent TensorBoard visualization tool. It even works (to an extent) if you’re using a backend other than TensorFlow, like Theano, or CNTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LambdaCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to create a basic one, you can use LambdaCallback. This allows you to trigger events when an epoch, batch, or training process begins or ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to create something a little more complex, you can create your own callback by inheriting from the keras.callbacks.Callback class. This is only for those who truly want low-level control over how a callback is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to solving this problem is to use early stopping. This involves monitoring the loss on the training dataset and a validation dataset (a subset of the training set not used to fit the model). As soon as loss for the validation set starts to show signs of overfitting, the training process can be stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping can be used with your model by first ensuring that you have a validation dataset. You can define the validation dataset manually via the validation_data argument to the fit() function, or you can use the validation_split and specify the amount of the training dataset to hold back for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T07:00:22.591501Z",
     "start_time": "2020-05-02T07:00:22.588429Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T07:00:54.730766Z",
     "start_time": "2020-05-02T07:00:54.707920Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=1)\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T07:02:18.458646Z",
     "start_time": "2020-05-02T07:02:17.665206Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T07:02:48.247580Z",
     "start_time": "2020-05-02T07:02:48.222217Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T07:03:16.086115Z",
     "start_time": "2020-05-02T07:03:16.083557Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T07:04:04.324415Z",
     "start_time": "2020-05-02T07:03:58.820923Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=200, batch_size=32, verbose=0, validation_split=0.3, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T07:05:22.636708Z",
     "start_time": "2020-05-02T07:05:22.632359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model started overfitting the data at 70th epoch\n"
     ]
    }
   ],
   "source": [
    "print('The model started overfitting the data at '+ str(len(history.history['loss']))+ 'th epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
